{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "austin_listings = pd.read_csv('./data/austin_listings.csv', low_memory=False)\n",
    "austin_reviews = pd.read_csv('./data/austin_reviews.csv', low_memory=False)\n",
    "sf_reviews = pd.read_csv('./data/sanfrancisco_reviews.csv', low_memory=False)\n",
    "sf_listings = pd.read_csv('./data/sanfrancisco_listings.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Austin listings dataframe shape\n",
    "\n",
    "austin_listings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check San Francisco listings dataframe shape\n",
    "\n",
    "sf_listings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine Austin listings and San Francisco listings dataframe\n",
    "\n",
    "result_listings = austin_listings.append(sf_listings)\n",
    "result_listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Austin reviews dataframe shape\n",
    "austin_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check San Francisco reviews dataframe shape\n",
    "\n",
    "sf_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the Austin and San Francisco dataframe reviews\n",
    "\n",
    "result_reviews = austin_reviews.append(sf_reviews)\n",
    "\n",
    "# drop id \n",
    "result_reviews = result_reviews.drop('id', axis=1)\n",
    "result_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename id column\n",
    "\n",
    "result_reviews.rename(columns={'listing_id':'id'}, inplace=True)\n",
    "print(result_reviews.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_reviews.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_listings.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_reviews = result_reviews.merge(result_listings, how=\"inner\", on='id' )\n",
    "comb_reviews = comb_reviews[['id', 'date', 'reviewer_name', 'reviewer_id', 'comments', 'review_scores_rating', 'review_scores_accuracy']]\n",
    "comb_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null_columns(df):\n",
    "    '''List all columns with missing values\n",
    "    \n",
    "    Input:\n",
    "        df: Dataframe\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: a dataframe with column names, number of missing values, and percentage of missing values\n",
    "    '''\n",
    "    cols = df.columns[df.isnull().sum() >= 0]\n",
    "    df_null = pd.DataFrame(df[cols].isnull().sum().sort_values(), columns=['Number of Nulls'])\n",
    "    df_null['% of Nulls'] = df[cols].isnull().mean().sort_values() * 100\n",
    "    \n",
    "    return df_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_null_columns(comb_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all null rows\n",
    "\n",
    "comb_reviews.dropna(inplace=True)\n",
    "comb_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null columns again\n",
    "\n",
    "check_null_columns(comb_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>What are property renters saying about their experiences in reviews ?</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert review score rating to integer\n",
    "\n",
    "comb_reviews[\"review_scores_rating\"] = comb_reviews[\"review_scores_rating\"].astype(int)\n",
    "comb_reviews[\"review_scores_rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiments(col):\n",
    "    '''\n",
    "    Function classify ratings to positive, negative and neutral\n",
    "    \n",
    "    Args:\n",
    "        col: the name of the column needed to be classified\n",
    "        \n",
    "    Returns:\n",
    "        string: classification status\n",
    "    '''\n",
    "    if (col > 50):\n",
    "        return \"Positive\"\n",
    "    elif col < 50:\n",
    "        return \"Negative\"\n",
    "    elif col == 50:\n",
    "        return \"Neutral\"\n",
    "    \n",
    "# Label the data sets with sentiments\n",
    "\n",
    "comb_reviews[\"class\"] = comb_reviews[\"review_scores_rating\"].apply(sentiments)\n",
    "comb_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = comb_reviews.groupby('class').size().reset_index(name=\"count\")\n",
    "sample_reviews['%count'] = sample_reviews['count']/sum(sample_reviews['count']) * 100\n",
    "\n",
    "\n",
    "sample_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize proportion of sentiments\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.barplot(x=\"class\", y=\"%count\", data=sample_reviews)\n",
    "ax.set_title('Airbnb Sentiment representation on reviews')\n",
    "plt.rcParams['figure.figsize']=(13,13)\n",
    "plt.savefig('./plots/rating_status.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_reviews.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a comment\n",
    "comb_reviews.iloc[0,4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sentiment Analysis</h2>\n",
    "\n",
    "We will extract some features to determine a comment's sentiment using the Bag of words strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Text preprocessing:</h3>\n",
    "<ul>\n",
    "<li>Tokenization (extracting words and sentences from input)</li>\n",
    "<li>Apply regex to remove punctuations, whitespaces from words</li>\n",
    "<li>Apply Stopwords to remove stopwords with nltk corpus </li>\n",
    "<li>Apply stemming For example: “Flying” is a word and its suffix is “ing”, if we remove “ing” from “Flying” then we will get base word or root word which is “Fly”.This suffix is used to create a new word from the original stem word.</li>\n",
    "<li>Apply lemmatization which links words with similar meaning to one word. </li>\n",
    "<li>Create bag of words with redundant words. </li>\n",
    "<li>Remove irrelevant words(filtering \"this\"and three lettered words). </li>\n",
    "<li>Counting occurence (builds a dictionary of features with popular words for each review in a list document)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def tokenize_words_Sents(sent):\n",
    "    '''Function to take text and split into words and sentences\n",
    "    \n",
    "    Input:\n",
    "        text: string\n",
    "\n",
    "    Returns:\n",
    "        List: tokenized words and sentences\n",
    "    '''\n",
    "    return word_tokenize(sent),sent_tokenize(sent)\n",
    " \n",
    "def regexp_tokenizer(sent):\n",
    "    '''Function to split sentences using regular expression(searches for groups that have alphanumerics and removes whitespaces)\n",
    "    \n",
    "    Input:\n",
    "        text: tokenized strings\n",
    "\n",
    "    Returns: \n",
    "        List: strings matched with either the tokens or the separators between tokens.\n",
    "    '''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(sent)\n",
    "\n",
    "words,sents = tokenize_words_Sents(comb_reviews.iloc[0,4])\n",
    "print(\"Words: \",words)\n",
    "print(\"Sents: \",sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stop_words(sent):\n",
    "    '''Function to remove stop words present in NLTK corpus from our list of words\n",
    "    \n",
    "    Input:\n",
    "        List: tokenized words\n",
    "\n",
    "    Returns:\n",
    "        List: 1. words not in nltk corpus\n",
    "         ''   2. words without stopwords\n",
    "    '''\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = []\n",
    "    for w in sent:\n",
    "        if w not in stop_words:\n",
    "            filtered_words.append(w)\n",
    "    return stop_words,filtered_words\n",
    "\n",
    "def remove_stop_word(sent):\n",
    "    '''Function to remove stop words present in NLTK corpus from our list of words\n",
    "    \n",
    "    Input:\n",
    "        List: tokenized words\n",
    "\n",
    "    Returns:\n",
    "        List: words not in stopwords\n",
    "    '''\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = []\n",
    "    for w in sent:\n",
    "        if w not in stop_words:\n",
    "            filtered_words.append(w)\n",
    "    return filtered_words\n",
    "\n",
    "stop_words,filtered_words = remove_stop_words(words)\n",
    "print(stop_words)\n",
    "print(\".................................Words.....................................\")\n",
    "print(words)\n",
    "print(\".................................Words filtered............................\")\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemming_words(words):\n",
    "    '''Function to shorten the lookup, and normalize sentences.\n",
    "    \n",
    "    Input:\n",
    "        List: tokenized words\n",
    "\n",
    "    Returns:\n",
    "        List: lemmatized words(root words)\n",
    "    '''\n",
    "    Ps = PorterStemmer()\n",
    "    stemmed_words = []\n",
    "    for m in words:\n",
    "        stemmed_words.append(Ps.stem(m))\n",
    "    return stemmed_words\n",
    "stemmed_words = stemming_words(filtered_words)\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatizing_words(words):\n",
    "    '''Function to replace synonyms.\n",
    "    \n",
    "    Input:\n",
    "        List: filtered words\n",
    "\n",
    "    Returns:\n",
    "        List: root words\n",
    "    '''\n",
    "    lemma = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    for root in words:\n",
    "        lemmatized_words.append(lemma.lemmatize(root))\n",
    "    return lemmatized_words\n",
    "\n",
    "lemmatized_words = lemmatizing_words(filtered_words)\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract words from comments and  Tokenize \n",
    "#RegExpTokenizer to use regex to eliminate punctuations\n",
    "\n",
    "list_words =  [regexp_tokenizer(m) for m in list(comb_reviews['comments'])]\n",
    "print(list_words[1], 'done tokenizing')\n",
    "\n",
    "#remove stop_words\n",
    "\n",
    "list_words =  [remove_stop_word(m) for m in list_words]\n",
    "print(list_words[1], 'done removing stop words')\n",
    "\n",
    "#Stemming\n",
    "\n",
    "list_words = [lemmatizing_words(m) for m in list_words]\n",
    "print(list_words[1], 'done lemmatizing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "def bag_of_words(list_words):\n",
    "    '''Function to model and represent frequent data.\n",
    "    \n",
    "    Input:\n",
    "        List: filtered words\n",
    "\n",
    "    Returns:\n",
    "        List: list of frequent words\n",
    "    '''\n",
    "    all_words = []\n",
    "    for m in list_words:\n",
    "        for w in m:\n",
    "            all_words.append(w.lower())\n",
    "    all_words = FreqDist(all_words)\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from wordcloud import WordCloud\n",
    "all_words = bag_of_words(list_words)\n",
    "ax = plt.figure(figsize=(15,10))\n",
    "\n",
    "# Generate a cloud image for frequent words\n",
    "wordcloud = WordCloud(background_color='white',max_font_size=40).generate(' '.join(all_words.keys()))\n",
    "\n",
    "# Display the generated image:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "print(\"Famous words:\",len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# visualize popular words\n",
    "from sklearn.manifold import TSNE\n",
    "all_words = bag_of_words(list_words)\n",
    "count = []\n",
    "words  = []\n",
    "for w in all_words.most_common(10):\n",
    "    count.append(w[1])\n",
    "    words.append(w[0])\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.barplot(words,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ListWords To lower case\n",
    "def remove_irrelevent_words(list_words):\n",
    "    '''Function to remove irrelevant words still contained after stopwords removal.\n",
    "    \n",
    "    Input:\n",
    "        List: filtered words\n",
    "\n",
    "    Returns:\n",
    "        List: words\n",
    "    '''\n",
    "    list_words1 = [] \n",
    "    for m in list_words:\n",
    "        l = [item.lower() for item in m]\n",
    "        list_words1.append(l)\n",
    "    list_words = list_words1\n",
    "    #elimnate words\n",
    "    for m in list_words:\n",
    "        for w in m:\n",
    "            if len(w) <=3:\n",
    "                m.remove(w)\n",
    "            if w == 'this':\n",
    "                m.remove(w)\n",
    "    return list_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figures inline and set visualization style\n",
    "list_words = remove_irrelevent_words(list_words)\n",
    "all_words = bag_of_words(list_words)\n",
    "print(\"All Words length \",len(all_words))\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "ax = plt.figure(figsize=(15,10))# Create freq distribution and plot\n",
    "freqdist1 = FreqDist(all_words)\n",
    "freqdist1.plot(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = bag_of_words(list_words)\n",
    "count = []\n",
    "words  = []\n",
    "for w in all_words.most_common(10):\n",
    "    count.append(w[1])\n",
    "    words.append(w[0])\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams['figure.figsize']=(12,12)\n",
    "sns.barplot(words,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def create_document(comb_reviews,list_words):\n",
    "    '''Function to create a document with a list of words and associated class.\n",
    "    \n",
    "    Input:\n",
    "        Dataframe: target class\n",
    "        List: preprocessed words\n",
    "\n",
    "    Returns:\n",
    "        Tuple: document\n",
    "    '''\n",
    "    list_class = list(comb_reviews['class'])\n",
    "    documents =  []\n",
    "    for m in range(len(list_words)):\n",
    "        documents.append((list_words[m],list_class[m]))\n",
    "    #shuffle\n",
    "    random.shuffle(documents)\n",
    "    return documents\n",
    "#Review docs\n",
    "documents = create_document(comb_reviews,list_words)\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document,all_words,num_of_words):\n",
    "    '''function to create a dictionary of features with popular words for each review in the list document.\n",
    "       The keys are the words in word_features. The values of each key are either true or false for \n",
    "       whether the feature appears in the review or not. \n",
    "    Input:\n",
    "       string: sample review\n",
    "       List: bag of words with the most repeated words\n",
    "       num_of_words: specific number of repeated words\n",
    "\n",
    "    Returns:\n",
    "        Dictionary: features with repeated words for each review, festures as keys and its existence as boolean\n",
    "    '''\n",
    "    most_comm_word = []    \n",
    "    for w in all_words.most_common(num_of_words):\n",
    "        most_comm_word.append(w[0])\n",
    "\n",
    "    word_features = most_comm_word\n",
    "    words = regexp_tokenizer(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "find_features(\"I really love the balcony\",all_words,150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We have been able to create features for each reviews. The system is able to identify features and establish a match from unseen data. The next thing we would have done is to create a model to classify this features into positive, negative and neutral reviews. However, this is beyond the scope of this project.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Airbnb reviews has a proportion of positive reviews than any other type of reviews(negative, neutral). Some of the most frequently used words by customers in comments are also visualized above. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airbnb-env",
   "language": "python",
   "name": "airbnb-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
